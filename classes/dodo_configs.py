from typing import TypedDict, List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, asdict, is_dataclass
from numpy import pi
import numpy as np


# -------------------------------------
# Use Dataclasses for Programming 
# They can be converted into normal dicts if necessary.
# -----------------------------------------------------------------------------

@dataclass
class DodoJointAngles:
    """
    Default joint angles of the Dodo robot (in radians) for each leg joint.
    """
    left_hip: float
    right_hip: float
    left_thigh: float
    right_thigh: float
    left_knee: float
    right_knee: float
    left_foot_ankle: float
    right_foot_ankle: float

@dataclass
class DodoJointNames:
    """
    Joint name mapping used by the simulator/URDF/MJCF for each Dodo leg joint.
    The Key is the naming convention, this programm is using.
    The values corresponding to the keys should be the joint names used in the URDF or the XML file!

    -> The class "FileFormatAndPaths" can be used to extract the joint names from those files!
    """
    left_hip: str
    right_hip: str
    left_thigh: str
    right_thigh: str
    left_knee: str
    right_knee: str
    left_foot_ankle: str
    right_foot_ankle: str

@dataclass
class DodoObservations:
    """
    Scaling factors applied to the different observation components.
    """
    lin_vel: float
    ang_vel: float
    dof_pos: float
    dof_vel: float

@dataclass
class RewardScales:
    """
    Per-term scaling factors that weight the individual reward components.
    """
    tracking_lin_vel: float
    tracking_ang_vel: float
    orientation_stability: float
    base_height: float
    survive: float
    fall_penalty: float
    vertical_stability: float
    periodic_gait: float
    foot_swing_clearance: float
    knee_extension_at_push: float
    bird_hip_phase: float
    forward_torso_pitch: float
    hip_abduction_penalty: float
    lateral_drift_penalty: float
    energy_penalty: float
    action_rate: float

@dataclass
class CommandRanges:
    """
    Sampling ranges [min, max] for the commanded base velocities.
    """
    lin_vel_x: List[float]
    lin_vel_y: List[float]
    ang_vel_yaw: List[float]

@dataclass
class TrainAlgorithm:
    """
    Defining the RL training algorithm and its hyperparameters.
    """
    class_name: str
    clip_param: float
    desired_kl: float
    entropy_coef: float
    gamma: float
    lam: float
    learning_rate: float
    max_grad_norm: float
    num_learning_epochs: int
    num_mini_batches: int
    schedule: str
    use_clipped_value_loss: bool
    value_loss_coef: float

@dataclass
class TrainPolicy:
    """
    Defining the RL training policy
    """
    activation: str
    actor_hidden_dims: List
    critic_hidden_dims: List
    init_noise_std: float
    class_name: str

@dataclass
class TrainRunner:
    """
    Defining the RL runner parameters
    """
    checkpoint: int
    experiment_name: str
    load_run: int
    log_interval: int
    max_iterations: int
    record_interval: int
    resume: bool
    resume_path: Any
    run_name: str


# -----------------------------------------------------------------------------
# Terrain Configs

@dataclass
class UnevenTerrainCfg:
    """
    Configuration for uneven terrain generation.
    """
    n_subterrains: Tuple[int, int]
    subterrain_size: Tuple[float, float]
    horizontal_scale: float
    vertical_scale: float
    spawn_flat_radius_sub: int
    border_flat: bool
    randomize: bool


@dataclass
class TerrainCfg:
    """
    Configuration for terrain generation.
    """
    # "plane" | "uneven" | "random"
    mode: str

    # Für mode="random" (leicht erweiterbar später)
    options: List[str]
    probs:   List[float]

    uneven: UnevenTerrainCfg

# -----------------------------------------------------------------------------
# Config-Classes 
# -----------------------------------------------------------------------------

@dataclass
class EnvCfg:
    """
    Configuration of the Dodo environment: robot model, init state and
    general simulation and control parameters used during training/evaluation.
    """
    num_actions: int
    default_joint_angles: DodoJointAngles
    joint_names_mapped: DodoJointNames
    kp: float
    kd: float
    termination_if_roll_greater_than: float
    termination_if_pitch_greater_than: float
    base_init_pos: List[float]
    base_init_quat: List[float]
    episode_length_s: float
    resampling_time_s: float
    action_scale: float
    simulate_action_latency: bool
    clip_actions: float
    robot_file_path: str
    foot_link_names: List[str]
    robot_file_format: str
    terrain_cfg: TerrainCfg


@dataclass
class ObsCfg:
    """
    Observation configuration: total observation size and scaling per component.
    """
    num_obs: int
    obs_scales: DodoObservations

@dataclass
class RewardCfg:
    """
    Reward configuration: scales for each term and shaping hyperparameters
    (targets and sigmas) used in the reward functions.
    """
    reward_scales: RewardScales
    tracking_sigma: float
    base_height_target: float
    height_sigma: float
    orient_sigma: float
    energy_sigma: float
    period: float
    clearance_target: float
    pitch_target: float
    pitch_sigma: float
    bird_hip_target: float
    bird_hip_amp: float
    bird_hip_sigma: float
    hip_abduction_sigma: float
    drift_sigma: float
    pitch_threshold: float
    roll_threshold: float
    base_height_threshold: float

@dataclass
class CommandCfg:
    """
    Command configuration: number of commands and how often as well as from
    which ranges they are (re-)sampled.
    """
    num_commands: int
    resampling_time_s: float
    command_ranges: CommandRanges


@dataclass
class TrainCfg:
    """
    Full training configuration used by the PPO trainer (rsl-rl).
    
    This config defines:
      • PPO algorithm hyperparameters (learning rates, clipping, KL targets)
      • Policy network architecture and initialization settings
      • Runner behavior (logging, checkpoints, iteration limits)
      • General training settings such as random seeds and normalization

    These parameters collectively determine how the agent is optimized, how
    rollouts are collected, and how model checkpoints and logs are saved.
    """
    algorithm: TrainAlgorithm
    init_member_classes: dict
    policy: TrainPolicy
    runner: TrainRunner
    runner_class_name: str
    num_steps_per_env: int
    save_interval: int
    empirical_normalization: Any
    seed: int
    logger: str
    tensorboard_subdir: str




# -----------------------------------------------------------------------------
# Helper Functions
# -----------------------------------------------------------------------------

def init_dodo_configs(
        exp_name: str, 
        max_iterations: int, 
        joint_names: list[str], 
        robot_file_path_relative: str,
        foot_link_names: list[str],
        robot_file_format: str,
        num_obs: int
        ) -> tuple[EnvCfg, ObsCfg, RewardCfg, CommandCfg, TrainCfg] :
    """
    This function can be used to directly create and return all configs that are relevant for the DODO training as dataclasses.
    You can easily create a dataclass object on your own and use that.
    This function can be used it should contain good values for training walking to the dodo.
    
    :param exp_name: Name of the experiment
    :type exp_name: str
    :param max_iterations: Max iterations for the RL training
    :type max_iterations: int
    :param joint_names: List of the joint names of the robot
    :type joint_names: list[str]
    :param robot_file_path_relative: Realtive path to your dodo robot urdf/xml file (starting from project root)
    :type robot_file_path_relative: str
    :param foot_link_names: List of the foot links of the robot
    :type foot_link_names: list[str]
    :param robot_file_format: Format of the robot while (usually "xml" or "urdf")
    :type robot_file_format: str
    :param num_obs: Number of observations that are made for the RL training.
    :type num_obs: int
    """
    
    train_config_dataclass: TrainCfg = TrainCfg(
        algorithm=TrainAlgorithm(
            class_name =                  "PPO",
            clip_param =                  0.2,
            desired_kl =                  0.02,    # <- vorher 0.01
            entropy_coef =                0.015,    # <- vorher 0.02
            gamma =                       0.99,    # <- vorher 0.98
            lam =                         0.95,
            learning_rate =               2e-4,     # <- vorher 2e-4
            max_grad_norm =               0.5,    # <- vorher 1.0
            num_learning_epochs =         5,      # <- vorher 8
            num_mini_batches =            16,        # split big batch into many mini-batches
            schedule =                    "adaptive",
            use_clipped_value_loss =      True,
            value_loss_coef =             0.5 # <- vorher 1.0
        ),
        init_member_classes={},
        policy=TrainPolicy(
            activation=                   "elu",
            actor_hidden_dims=            [512, 256, 128],
            critic_hidden_dims=           [512, 256, 128],
            init_noise_std=               0.15,     # <- vorher 0.25
            class_name=                   "ActorCritic"
        ),
        runner=TrainRunner(
        checkpoint=                     -1,
        experiment_name=                exp_name,
        load_run=                       -1,
        log_interval=                    5,
        max_iterations=                 max_iterations,
        record_interval=                -1,
        resume=                         False,
        resume_path=                    None,
        run_name=                       ""
        ),
        runner_class_name=                "OnPolicyRunner",
        # collect at least one gait cycle per env: e.g. 1.0s / dt(0.01) = 100 steps
        num_steps_per_env=                64, # vorher 256 oder 192
        save_interval=                    50,
        empirical_normalization=          True,
        seed=                             1,
        logger=                           "wandb",
        tensorboard_subdir=               "tb"
    )


    terrain_config_dataclass: TerrainCfg = TerrainCfg(
        mode="plane",
        options=["plane", "uneven"],
        probs=[0.5, 0.5],
        uneven=UnevenTerrainCfg(
            n_subterrains=(16, 16),
            subterrain_size=(2.0, 2.0),
            horizontal_scale=0.25, # Add curriculum later
            vertical_scale=0.003,
            spawn_flat_radius_sub=0, # 0 is for just one flat subterrain in the center. 1 is for 3x3 flat subterrains etc.
            border_flat=True,
            randomize=True
        )
    )
    
    env_config_dataclass: EnvCfg = EnvCfg(
        num_actions=                      len(joint_names),
        default_joint_angles=DodoJointAngles(
            left_hip=                     0.00,
            right_hip=                    0.00,
            left_thigh=                   0.4,
            right_thigh=                  0.4,
            left_knee=                    -0.7,
            right_knee=                   -0.7,
            left_foot_ankle=              0.3,
            right_foot_ankle=             0.3
        ),
        joint_names_mapped=DodoJointNames( # The values should be the joint_names from the robot file (URDF or XML) you can hardcode it here if you want to. I am using the joint names that are automatically extracted by the helper function.
            left_hip=                     joint_names[4],
            right_hip=                    joint_names[0],
            left_thigh=                   joint_names[5],
            right_thigh=                  joint_names[1],
            left_knee=                    joint_names[6],
            right_knee=                   joint_names[2],
            left_foot_ankle=              joint_names[7],
            right_foot_ankle=             joint_names[3],
        ),
        kp=                               100.0,
        kd=                               2.0 * np.sqrt(100.0), #== 2.0 * np.sqrt(150.0)
        termination_if_roll_greater_than= 30.0,
        termination_if_pitch_greater_than=30.0,
        base_init_pos=                    [0.0, 0.0, 0.55],
        base_init_quat=                   [1.0, 0.0, 0.0, 0.0],
        episode_length_s=                 10.0,
        resampling_time_s=                2.0,
        action_scale=                     0.5,
        simulate_action_latency=          False,
        clip_actions=                     1.0, # war 100 -> sinnvoll clampen
        robot_file_path=                  robot_file_path_relative, # for example: "robot_mjcf": dodo_robot\dodo.xml
        foot_link_names=                  foot_link_names, # for example: ['Left_FOOT_FE', 'Right_FOOT_FE']
        robot_file_format=                robot_file_format,
        terrain_cfg=                      terrain_config_dataclass
    )    
    
    obs_config_dataclass: ObsCfg = ObsCfg(
        num_obs=                          num_obs,
        obs_scales=DodoObservations(
            lin_vel=                      2.0,
            ang_vel=                      0.4,
            dof_pos=                      1.0,
            dof_vel=                      0.1
        )
    )

    reward_config_dataclass: RewardCfg = RewardCfg(
        reward_scales=RewardScales(
            #velocity tracking
            tracking_lin_vel=             3.0,
            tracking_ang_vel=             1.0,
            #stability and posture
            orientation_stability=        1.0,
            base_height=                  2.0,
            survive=                      0.5,
            fall_penalty=                 40.0,
            vertical_stability=           0.05,  # oder 0.05 zum Start. hüpfen
            #gait-shaping (bird style)
            periodic_gait=                0.8,
            foot_swing_clearance=         1.5,
            knee_extension_at_push=       0.1,
            bird_hip_phase=               0.5,
            forward_torso_pitch=          0.1,
            #Joint penalties
            hip_abduction_penalty=        0.01,
            #drift and efficiency
            lateral_drift_penalty=        0.0, # drift in x richtung 
            action_rate=                  0.15, # Definiere eine Funktion, die dafür sorgt, dass die gesampleten aktionen nicht zu weit von den vorigen abweichen (smoother trajectory).
            energy_penalty=               0.0,
        ),
        # Hyperparameter für die Gauß‑Formen und Targets
        tracking_sigma=                   0.2,
        base_height_target=               0.55,
        height_sigma=                     0.10,   # Hüfthöhe
        orient_sigma=                     0.10,   # Roll/Pitch
        energy_sigma=                     0.35,   # Aktionsänderung
        period=                           1.00,   # Zyklusdauer in s
        clearance_target=                 0.08,   # m, min. Fußhöhe im Swing
        pitch_target=                     0.15,   # rad (~10°), leichter Vorwärts‑Pitch
        pitch_sigma=                      0.10,   # Breite für Pitch‑Reward
        bird_hip_target=                 -0.7,   # rad (~20°) Hüft‑FE‑Baseline nach hinten
        bird_hip_amp=                     0.35,   # rad (~8°) Zyklus‑Amplitude
        bird_hip_sigma=                   0.30,   # Breite des Hüft‑Phase‑Rewards
        hip_abduction_sigma=              0.2,   # Breite für Hüft‑AA‑Penalty
        drift_sigma=                      0.15,   # Breite für seitliche Drift
        pitch_threshold=                  40 * pi/180,
        roll_threshold=                   40 * pi/180,
        base_height_threshold=            0.38   # If the base height gets lower than that the robot is considered fallen.
    )

    command_config_dataclass: CommandCfg = CommandCfg(
        num_commands= 3,
        resampling_time_s= 2.0,
        command_ranges=CommandRanges(
            lin_vel_x=[0.1, 0.7],
            lin_vel_y=[-0.0, 0.0], # Geradeaus
            ang_vel_yaw=[0.0, 0.0] # for example [-1.0, 1.0]
        )
    )


    return env_config_dataclass, obs_config_dataclass, reward_config_dataclass, command_config_dataclass, train_config_dataclass


def dataclass_to_dict(**kwargs) -> Dict[str, Any]:
    """
    Convert multiple dataclass instances into dicts and return them
    in the same order as provided in the function call.

    Usage:
        env_cfg, obs_cfg, reward_cfg = dataclass_to_dict(
            env_cfg=env,
            obs_cfg=obs,
            reward_cfg=reward,
        )
    """
    result = []

    for name, obj in kwargs.items():
        if not is_dataclass(obj):
            raise TypeError(f"Argument '{name}' is not a dataclass instance")
        result.append(asdict(obj))

    # return as tuple so it can be unpacked
    return tuple(result)


# (env_config_dataclass,
# obs_config_dataclass,
# reward_config_dataclass,
# command_config_dataclass,
# train_config_dataclass) = init_dodo_configs(
#     exp_name="test",
#     foot_link_names=["dd", "dd", "dd", "dd", "dd", "dd"],
#     joint_names=["dd", "dd", "dd", "dd", "dd", "dd", "dd", "dd", "dd", "dd", "dd", "dd"],
#     max_iterations=2,
#     num_obs=3,
#     robot_file_format="dd",
#     robot_file_path_relative="dd",
# )

# (env_cfg,
# obs_cfg,
# reward_cfg,
# command_cfg,
# train_cfg,) = dataclass_to_dict(
#     env_cfg = env_config_dataclass,
#     obs_cfg = obs_config_dataclass,
#     reward_cfg = reward_config_dataclass,
#     command_cfg = command_config_dataclass,
#     train_cfg = train_config_dataclass,
# )